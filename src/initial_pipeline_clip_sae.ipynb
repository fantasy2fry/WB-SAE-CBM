{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b399db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from pathlib import Path\n",
    "from sparse_autoencoder import SparseAutoencoder\n",
    "\n",
    "\n",
    "def process_image_pipeline(image_path, sae_model_path, output_path):\n",
    "    \"\"\"\n",
    "    Przetwarza obraz przez model CLIP i SAE, a następnie zapisuje wynik.\n",
    "    :param image_path: Ścieżka do obrazu wejściowego.\n",
    "    :param sae_model_path: Ścieżka do wytrenowanego modelu SAE.\n",
    "    :param output_path: Ścieżka do zapisu przetworzonych cech.\n",
    "    \"\"\"\n",
    "\n",
    "    # Wybór urządzenia\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # CLIP\n",
    "    model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "    # Załaduj i przetwórz obraz\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    # Przetwarzanie obrazu przez CLIP\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "\n",
    "    # SAE\n",
    "    def load_sae_model(sae_checkpoint_path):\n",
    "        state_dict = torch.load(sae_checkpoint_path, map_location=device)\n",
    "        autoencoder_input_dim = 768  # CLIP ViT-L/14\n",
    "        expansion_factor = 8\n",
    "        n_learned_features = int(autoencoder_input_dim * expansion_factor)\n",
    "        len_hook_points = 1  \n",
    "\n",
    "        sae = SparseAutoencoder(\n",
    "            n_input_features=autoencoder_input_dim,\n",
    "            n_learned_features=n_learned_features,\n",
    "            n_components=len_hook_points\n",
    "        ).to(device)\n",
    "\n",
    "        sae.load_state_dict(state_dict)\n",
    "        sae.eval()\n",
    "        return sae  \n",
    "\n",
    "    # Przepuszczanie CLIP features przez SAE\n",
    "    @torch.no_grad()\n",
    "    def get_sae_representation(clip_features, sae_model):\n",
    "        concepts, _ = sae_model(clip_features)\n",
    "        return concepts\n",
    "\n",
    "\n",
    "    sae = load_sae_model(sae_model_path)\n",
    "    sae_repr = get_sae_representation(image_features, sae)\n",
    "\n",
    "    # Zapisz wynik\n",
    "    torch.save(sae_repr.cpu(), output_path)\n",
    "    print(f\"Zapisano: {output_path}\")\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3ceb44a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zapisano: dog_sae_concepts.pth\n"
     ]
    }
   ],
   "source": [
    "process_image_pipeline(\"dog.jpeg\", \"clip_ViT-L_14sparse_autoencoder_final.pt\", \"dog_sae_concepts.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882a4cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
