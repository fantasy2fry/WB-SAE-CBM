{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea7fbd0",
   "metadata": {},
   "source": [
    "Przepuszczamy wszystkie obrazki z binary_dataset_pelican przez pipeline z pliku initial_pipeline_sae.ipynb a wstawiamy reprezentacje wyrzucone przez sae do jednego datasetu wraz z labelami (y oznacza 1 jeśli na zdjęciu jest pelican i 0 jeśli nie ma)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1751c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "from pathlib import Path\n",
    "from sparse_autoencoder import SparseAutoencoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def process_image_pipeline(image_path, sae_model_path):\n",
    "    \"\"\"\n",
    "    Przetwarza obraz przez model CLIP i SAE, a następnie zapisuje wynik.\n",
    "    :param image_path: Ścieżka do obrazu wejściowego.\n",
    "    :param sae_model_path: Ścieżka do wytrenowanego modelu SAE.\n",
    "    :param output_path: Ścieżka do zapisu przetworzonych cech.\n",
    "    \"\"\"\n",
    "\n",
    "    # Wybór urządzenia\n",
    "    device = \"cuda\" if torch.cuda.is_available() else ('mps' if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Używane urządzenie: {device}\")\n",
    "\n",
    "    # CLIP\n",
    "    model, preprocess = clip.load(\"ViT-L/14\", device=device)\n",
    "    # Załaduj i przetwórz obraz\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    # Przetwarzanie obrazu przez CLIP\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "\n",
    "    # SAE\n",
    "    def load_sae_model(sae_checkpoint_path):\n",
    "        state_dict = torch.load(sae_checkpoint_path, map_location=device)\n",
    "        autoencoder_input_dim = 768  # CLIP ViT-L/14\n",
    "        expansion_factor = 8\n",
    "        n_learned_features = int(autoencoder_input_dim * expansion_factor)\n",
    "        len_hook_points = 1  \n",
    "\n",
    "        sae = SparseAutoencoder(\n",
    "            n_input_features=autoencoder_input_dim,\n",
    "            n_learned_features=n_learned_features,\n",
    "            n_components=len_hook_points\n",
    "        ).to(device)\n",
    "\n",
    "        sae.load_state_dict(state_dict)\n",
    "        sae.eval()\n",
    "        return sae  \n",
    "\n",
    "    # Przepuszczanie CLIP features przez SAE\n",
    "    @torch.no_grad()\n",
    "    def get_sae_representation(clip_features, sae_model):\n",
    "        concepts, _ = sae_model(clip_features)\n",
    "        return concepts\n",
    "\n",
    "\n",
    "    sae = load_sae_model(sae_model_path)\n",
    "    sae_repr = get_sae_representation(image_features, sae)\n",
    "\n",
    "    return sae_repr.cpu().squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1102f66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e2d7a9",
   "metadata": {},
   "source": [
    "Zróbmy (przynajmniej na razie) modele tylko na 100 zdjęciach z konceptem i 100 bez bo za długo to się wykonuje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baa0eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_concept_dataset(dataset_path, sae_model_path, concept_name, max_per_class=100):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    for label_dir in [\"0_other\", f\"1_{concept_name}\"]:\n",
    "        label = 0 if label_dir == \"0_other\" else 1\n",
    "        dir_path = Path(dataset_path) / label_dir\n",
    "        image_paths = list(dir_path.glob(\"*.jpg\")) + list(dir_path.glob(\"*.png\")) + list(dir_path.glob(\"*.jpeg\"))\n",
    "        \n",
    "        # Bierzemy tylko do max_per_class przykładów\n",
    "        image_paths = image_paths[:max_per_class]\n",
    "\n",
    "        for img_path in tqdm(image_paths, desc=f\"Processing {label_dir}\"):\n",
    "            try:\n",
    "                vec = process_image_pipeline(img_path, sae_model_path) \n",
    "                X.append(vec.squeeze().numpy())\n",
    "                y.append(label)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "    # Konwersja do tensora\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.int64)\n",
    "\n",
    "    # Zapis\n",
    "    torch.save(X_tensor, \"X_concepts.pt\")\n",
    "    torch.save(y_tensor, \"y_labels.pt\")\n",
    "\n",
    "    return X_tensor, y_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ec8f0270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Używane urządzenie: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 0_other:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Używane urządzenie: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 0_other:   1%|          | 1/100 [00:06<10:07,  6.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing ..\\WB-SAE-CBM\\concept_datasets\\binary_dataset_pelican\\0_other\\n01440764_11444.JPEG: bad allocation\n",
      "Używane urządzenie: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 0_other:   2%|▏         | 2/100 [00:11<09:18,  5.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing ..\\WB-SAE-CBM\\concept_datasets\\binary_dataset_pelican\\0_other\\n01440764_11974.JPEG: bad allocation\n",
      "Używane urządzenie: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing 0_other:   2%|▏         | 2/100 [00:16<13:13,  8.10s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_concept_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m../WB-SAE-CBM/concept_datasets/binary_dataset_pelican\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclip_ViT-L_14sparse_autoencoder_final.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpelican\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[21], line 16\u001b[0m, in \u001b[0;36mcreate_concept_dataset\u001b[1;34m(dataset_path, sae_model_path, concept_name, max_per_class, save_each)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, img_path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tqdm(image_paths, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 16\u001b[0m         vec \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_image_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msae_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m         X_list\u001b[38;5;241m.\u001b[39mappend(vec\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     18\u001b[0m         y_list\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "Cell \u001b[1;32mIn[19], line 24\u001b[0m, in \u001b[0;36mprocess_image_pipeline\u001b[1;34m(image_path, sae_model_path)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUżywane urządzenie: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# CLIP\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m model, preprocess \u001b[38;5;241m=\u001b[39m \u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mViT-L/14\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Załaduj i przetwórz obraz\u001b[39;00m\n\u001b[0;32m     26\u001b[0m image \u001b[38;5;241m=\u001b[39m preprocess(Image\u001b[38;5;241m.\u001b[39mopen(image_path))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\Dell\\Desktop\\sem6\\wb\\Discover-then-Name\\clip\\clip.py:133\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, device, jit, download_root)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a CLIP model\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \n\u001b[0;32m    110\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;124;03m    A torchvision transform that converts a PIL image into a tensor that the returned model can take as its input\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _MODELS:\n\u001b[1;32m--> 133\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m \u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_MODELS\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpanduser\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m~/.cache/clip\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(name):\n\u001b[0;32m    136\u001b[0m     model_path \u001b[38;5;241m=\u001b[39m name\n",
      "File \u001b[1;32mc:\\Users\\Dell\\Desktop\\sem6\\wb\\Discover-then-Name\\clip\\clip.py:55\u001b[0m, in \u001b[0;36m_download\u001b[1;34m(url, root)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdownload_target\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m exists and is not a regular file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(download_target):\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hashlib\u001b[38;5;241m.\u001b[39msha256(\u001b[38;5;28mopen\u001b[39m(download_target, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mread())\u001b[38;5;241m.\u001b[39mhexdigest() \u001b[38;5;241m==\u001b[39m expected_sha256:\n\u001b[0;32m     56\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m download_target\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X, y = create_concept_dataset(\"../WB-SAE-CBM/concept_datasets/binary_dataset_pelican\", \"clip_ViT-L_14sparse_autoencoder_final.pt\", \"pelican\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c75d943",
   "metadata": {},
   "source": [
    "Model liniowy na neuronie odpowiadającym za koncept pelikana:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf8c99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69e7d1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Podział danych\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# neuron dla \"pelican\" \u001b[39;00m\n\u001b[0;32m      5\u001b[0m pelican_neuron \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1085\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\saeenv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\saeenv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2660\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2657\u001b[0m arrays \u001b[38;5;241m=\u001b[39m indexable(\u001b[38;5;241m*\u001b[39marrays)\n\u001b[0;32m   2659\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m-> 2660\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_shuffle_split\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2661\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault_test_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.25\u001b[39;49m\n\u001b[0;32m   2662\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2664\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[0;32m   2665\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stratify \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Dell\\anaconda3\\envs\\saeenv\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2308\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2305\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(n_train), \u001b[38;5;28mint\u001b[39m(n_test)\n\u001b[0;32m   2307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2308\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2309\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWith n_samples=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, test_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m and train_size=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2310\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2311\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maforementioned parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2312\u001b[0m     )\n\u001b[0;32m   2314\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "# Podział danych\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# neuron dla \"pelican\" \n",
    "pelican_neuron = 1085\n",
    "\n",
    "# Model\n",
    "X_train_single = X_train[:, pelican_neuron].reshape(-1, 1)\n",
    "X_test_single = X_test[:, pelican_neuron].reshape(-1, 1)\n",
    "clf_single = LogisticRegression().fit(X_train_single, y_train)\n",
    "preds_single = clf_single.predict(X_test_single)\n",
    "acc_single = accuracy_score(y_test, preds_single)\n",
    "roc_single = roc_auc_score(y_test, clf_single.predict_proba(X_test_single)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e255766",
   "metadata": {},
   "source": [
    "Regresja logistyczna dla wszystkich neuronów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b216feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "clf_full = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "preds_full = clf_full.predict(X_test)\n",
    "acc_full = accuracy_score(y_test, preds_full)\n",
    "roc_full = roc_auc_score(y_test, clf_full.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8c2b58",
   "metadata": {},
   "source": [
    "Wyniki: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4644bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pelican_results = {\n",
    "    \"naming_neuron\": {\"accuracy\": acc_single, \"roc_auc\": roc_single},\n",
    "    \"logistic_regression_full\": {\"accuracy\": acc_full, \"roc_auc\": roc_full}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1219fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Pelican results: {pelican_results}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01448fad",
   "metadata": {},
   "source": [
    "To samo dla 2 pozostałych konceptów: flamingi i gęsi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8556bdb3",
   "metadata": {},
   "source": [
    "Flamingi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe34fbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_flamingo, y_flamingo = create_concept_dataset(\"../WB-SAE-CBM/concept_datasets/binary_dataset_flamingos\", \"clip_ViT-L_14sparse_autoencoder_final.pt\", \"flamingo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ab763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podział danych\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_flamingo, y_flamingo, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# neuron dla \"flamingo\" \n",
    "flamingo_neuron = 2347\n",
    "\n",
    "# Model na pojedynczym neuronie\n",
    "X_train_single = X_train[:, flamingo_neuron].reshape(-1, 1)\n",
    "X_test_single = X_test[:, flamingo_neuron].reshape(-1, 1)\n",
    "clf_single = LogisticRegression().fit(X_train_single, y_train)\n",
    "preds_single = clf_single.predict(X_test_single)\n",
    "acc_single = accuracy_score(y_test, preds_single)\n",
    "roc_single = roc_auc_score(y_test, clf_single.predict_proba(X_test_single)[:, 1])\n",
    "\n",
    "# Model na pełnym wektorze\n",
    "clf_full = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "preds_full = clf_full.predict(X_test)\n",
    "acc_full = accuracy_score(y_test, preds_full)\n",
    "roc_full = roc_auc_score(y_test, clf_full.predict_proba(X_test)[:, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bdc3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_flamingo = {\n",
    "    \"naming_neuron\": {\"accuracy\": acc_single, \"roc_auc\": roc_single},\n",
    "    \"logistic_regression_full\": {\"accuracy\": acc_full, \"roc_auc\": roc_full}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552769e9",
   "metadata": {},
   "source": [
    "Gęsi:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3704c0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_geese, y_geese = create_concept_dataset(\"../WB-SAE-CBM/concept_datasets/binary_dataset_geese\", \"clip_ViT-L_14sparse_autoencoder_final.pt\", \"goose\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2e3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podział danych\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_geese, y_geese, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# neuron dla \"goose\" \n",
    "goose_neuron = 3426\n",
    "\n",
    "# Model na pojedynczym neuronie\n",
    "X_train_single = X_train[:, goose_neuron].reshape(-1, 1)\n",
    "X_test_single = X_test[:, goose_neuron].reshape(-1, 1)\n",
    "clf_single = LogisticRegression().fit(X_train_single, y_train)\n",
    "preds_single = clf_single.predict(X_test_single)\n",
    "acc_single = accuracy_score(y_test, preds_single)\n",
    "roc_single = roc_auc_score(y_test, clf_single.predict_proba(X_test_single)[:, 1])\n",
    "\n",
    "# Model na pełnym wektorze\n",
    "clf_full = LogisticRegression(max_iter=1000).fit(X_train, y_train)\n",
    "preds_full = clf_full.predict(X_test)\n",
    "acc_full = accuracy_score(y_test, preds_full)\n",
    "roc_full = roc_auc_score(y_test, clf_full.predict_proba(X_test)[:, 1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4d38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_goose = {\n",
    "    \"naming_neuron\": {\"accuracy\": acc_single, \"roc_auc\": roc_single},\n",
    "    \"logistic_regression_full\": {\"accuracy\": acc_full, \"roc_auc\": roc_full}\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saeenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
